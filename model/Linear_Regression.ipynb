{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3ec2daf",
   "metadata": {},
   "source": [
    "# LINEAR REGRESSION "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2488c407",
   "metadata": {},
   "source": [
    "## CHAPTER #1 - LINEAR REGRESSION MODEL "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923bad8d",
   "metadata": {},
   "source": [
    " ### CLASS CREATION  \n",
    " This class will contain all of the model logic.   \n",
    " The class contains the Linear regression model itself. \n",
    " Each object is a \"best fit line\"  i.e a line for each set of training data.   \n",
    "\n",
    " The attributes contain the:  \n",
    " - weight (coefficeint of the independent variable, x)  \n",
    " - bias (y-intercept) of the line \n",
    "\n",
    " The methods contain all the \"verbs\" of the model (functions that change the weight and bias), namely:  \n",
    " - how the model learns the weight and bias. \n",
    " - how the model fits the line to the data.  \n",
    " - how the line is evaluated e.g R^2 and RMSE. \n",
    " - how close our predictions are i.e residuals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdeb379",
   "metadata": {},
   "outputs": [],
   "source": [
    "class  LinearRegressionOld(object):\n",
    "    def __init__(self, learning_rate, epochs, weight=0, bias=0, ): #initialises the attributes of the class at 0\n",
    "        self.weight = weight              #stores weight \n",
    "        self.bias = bias                  #stores bias \n",
    "        self.x = []                       #creates empty list to store our predictor variables (x)  \n",
    "        self.y = []                       #creates empty list to store our our predicted variables (y) \n",
    "        self.learning_rate = learning_rate #setting your own learning rate \n",
    "        self.epochs = int(epochs )              #setting your own number of epochs \n",
    " \n",
    "    def vectorise(self, x,y):             #defining method to store the data points to be modelled\n",
    "        self.x = x                  #storing the values of x (independent variable) within the class \n",
    "        self.y = y                  #storing the values of y (dependent variable) within the class \n",
    "\n",
    "    def predict_y (self):                 #calculating the predicted y[i] for our optimisation later \n",
    "        y_predict =[]                     #creating an empty list to store all predicted y values \n",
    "        n = len(self.y)                   #range that we iterate over (number of values of y)\n",
    "\n",
    "        for i in range(n):                #looping over the number of values we have in the dataset \n",
    "            y_predict.append(self.weight*self.x[i] + self.bias)     #calculating predicted y values with line equation and adding predicted values to our list \n",
    "        return y_predict                  \n",
    "\n",
    "#NUMERICAL OPTIMISATION \n",
    "#Creating method to get weight\n",
    "    def partial_w(self):                  #partial derivative in regard to weight \n",
    "        y_predict = self.predict_y()      #predicted y value is equal to calling the internal method we defined above \n",
    "        gradient = 0 \n",
    "        n=len(self.y)\n",
    "\n",
    "        for i in range(n):\n",
    "            gradient += self.x[i]*(y_predict[i] - self.y[i])         #partial derivative equation to calculate total partial derivative of weight in regards to error function\n",
    "        return (-2/n)*gradient                                       #returns the  weight eqaution that minimises the partial derivative in regard to error function\n",
    "\n",
    "#Creating method to get bias \n",
    "    def partial_b(self):\n",
    "        y_predict = self.predict_y()\n",
    "        gradient = 0\n",
    "        n=len(self.y)\n",
    "\n",
    "        for i in range(n):\n",
    "            gradient += (y_predict[i]- self.y[i])                     #partial derivative equation to calculate total partial derivative of bias in regards to error function\n",
    "        return (-2/n)*gradient                                        #returns the  bias equation that minimises the partial derivative in regard to error function\n",
    "\n",
    "#Gradient Descent - iterating over multiple steps with our partial weight and bias functions \n",
    "    def optimise(self): \n",
    "        learn_rate = self.learning_rate                #size of steps we make \"downhill\" to minimise total error in regards to the weight and bias \n",
    "\n",
    "        for i in range(self.epochs):             #number of \"epochs\"/ steps we take in order to minimise aggregate error \n",
    "            self.weight = self.weight + learn_rate * self.partial_w() #optimised weight by calling partial_w 10000 times\n",
    "            self.bias = self.bias + learn_rate * self.partial_b()     #optimised bias  by calling partial_b 10000 times\n",
    "            if i % 10 == 0:                #prints out the weight and bias every 10 epochs \n",
    "                print(self.weight, self.bias)\n",
    "    \n",
    "#Residuals - creating a new residuals method to display deviation of predicted values from actual values\n",
    "    def residuals(self):\n",
    "        residuals = []\n",
    "        n=len(self.x) \n",
    "\n",
    "        for i in range(n):\n",
    "            residuals.append(self.y[i] - (self.weight * self.x[i] + self.bias)) #adding to the list called \"residuals\" the difference between actual and predicted y\n",
    "        return residuals                                                        \n",
    "\n",
    "#EVALUATION METRICS  -  these are key values that we will use to quantify how good our model predicts the data it is trained on. \n",
    "#Mean Square Error (MSE)  - the average squared deviation from actual values of y\n",
    "\n",
    "    def mse(self):\n",
    "        mse = 0                              #initialising our mse as a variable  that will be updated through the loops \n",
    "        mse_list = []                        #empty list to store our mse\n",
    "        n=len(self.y)                        #creating length for range to iterate over\n",
    "        total_error = []                     #what is the total error i.e actual - predicted y\n",
    "        self.square_error = 0                #stores the square error of the deviations \n",
    "\n",
    "        for i in range(n):                   #iterating to calculate the mse \n",
    "            total_error.append((self.y[i] - (self.weight * self.x[i] + self.bias))**2) #deviation from actual y ^2\n",
    "            self.square_error += total_error[i] #storing square error to be used in future calculations\n",
    "            mse_list.append((1/n)*total_error[i]) #storing mse in the list using the mean squared error formula \n",
    "            mse += mse_list[i]               #iterates by adding all elements in the list together to give us our aggreagte mse\n",
    "        return mse \n",
    "    \n",
    "#R^2 -  how much of the deviation in y is explained by our model\n",
    "    def rsquared(self):\n",
    "        n=len(self.y)\n",
    "        self.avg_y = 0                       #initial value of the average of our actual y values \n",
    "\n",
    "    #Average y- average of our actual y    \n",
    "        for i in range(n):\n",
    "            self.avg_y += ((1/n)*self.y[i])  #calculating the average value of actual y \n",
    "        \n",
    "    #Total sum of squares -  \n",
    "        self.sum_squares = 0                 #creating an object called sum_squares to be used further in the function \n",
    "        sum_squares_list =[]                 #empty list to store values of sum of squares \n",
    "        n = len(self.y)\n",
    "\n",
    "        for i in range(n):\n",
    "            sum_squares_list.append((self.y[i] - self.avg_y)**2) #the squared values of actual - predicted y  and storing them in the empty list above \n",
    "            self.sum_squares += sum_squares_list[i]              #adding togther all of the sum of squares into initial variable sum_squares \n",
    "\n",
    "    #Final calculation \n",
    "        rsquared = 0                         #initialising our value of rsquared as 0 \n",
    "        n=len(self.y)\n",
    "\n",
    "        rsquared = (1-(self.square_error/self.sum_squares)) #calculating R^2 with our instances of sum of squares and square error \n",
    "        return rsquared \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244a6b2b",
   "metadata": {},
   "source": [
    "EXAMPLE WITH SIMPLE LISTS\n",
    "Now I have added arguments for adjustable learning rates and epochs numbers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd657916",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = (1,2,3,4,5)\n",
    "y = (6,7,8,9,10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2850f30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install matplotlib\n",
    "%pip install pandas\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a664c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965ff661",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegressionOld(0.05, 10000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866e5ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.vectorise(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e83ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953ad096",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_y()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd4758d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.residuals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa3d6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.mse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d6dd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.rsquared()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f69169",
   "metadata": {},
   "source": [
    "## IMPLEMENTATION WITH DIIFFERENT TYPES\n",
    "Upon further inspection I realised that a big problem that would reveal itself is that when gradient descent runs, different types of  x and y values can make the model crash.  \n",
    "\n",
    "For example, data in vectorise() taken as dataframes cannot work as the dimensions are incorrect and there are headers and indexes. Therefore I need a method to normalise all of the input of dependent and independt variables i.e turn them all into iterable, indexable and numerical compund types like lists.\n",
    "\n",
    "Thankfully, that is what my vectorise() method is for. I just need to modify it so that:  \n",
    "1) It takes in the values we want to model whether they be pandas series, lists, tuples etc\n",
    "2) It checks what types our inputs are coming in as. \n",
    "   -  If they are iterable and only numeric e.g lists, tuples, numpy arrays - great \n",
    "   -  If not iterable or not only numeric - not great and needs to be wrapped into iterable like a list and made only numeric\n",
    "3) Ensure data in itrable like lists is numeric and not strings \n",
    "4) Store these iterables  as self.x and self.y "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fc529a",
   "metadata": {},
   "source": [
    "## HOW DO WE CHECK TYPES OF INPUTS?\n",
    "\n",
    "We require iterables of the type that can:\n",
    "1) Can be indexed\n",
    "2) Are numeric   \n",
    "- This leaves us with valid data types of **lists, tuples, numpy arrays and pandas series**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d33feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340f1281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectoriseold(x):\n",
    "    if isinstance(x, (tuple, list, np.ndarray, pd.Series, pd.DataFrame)):\n",
    "        return \"Ready to go!\"\n",
    "    else:\n",
    "        return \"Just a moment\"\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da36fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = (10,20,30) #Trial with a tuple\n",
    "vectoriseold(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e80a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [30,40,50] #Trial with a list\n",
    "vectoriseold(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac2a53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Weight':[136.3,140.5,155.2,180.0], #Trial with a dataframe \n",
    "                  'Age':[20,23,27,31]}) \n",
    "vectoriseold(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d65889",
   "metadata": {},
   "source": [
    "## CONVERTING ITERABLES INTO LISTS \n",
    "Now that we can check if our input is an interable like we want, let us convert them into common types.\n",
    "I will use Python base types of a list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d6f5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorisenewer(x):\n",
    "    if isinstance(x, (tuple, list, np.ndarray, pd.Series)): #removed pd.Dataframe since our linear regression handles n x 1 features so dataframes are incorrect dimension.\n",
    "        x = list(x)\n",
    "        return \"Ready to go!\"\n",
    "    else:\n",
    "        print(\"Just a moment\")\n",
    "        x = [x]\n",
    "        print(\"Ready to go!\")\n",
    "    return x\n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67575a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[['Weight']]\n",
    "print(df1)\n",
    "vectorisenewer(df1) #works but returns entire dataframe as list with heading and index. \n",
    "                    #need to remove all metadata from the pandas dataframe \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62502db9",
   "metadata": {},
   "source": [
    "## DEALING WITH PANDAS SERIES AND DATAFRAMES\n",
    "So far our iterables are always just numeric values that can easily be converted into lists.\n",
    "Pandas dataframes are different. They are lablled data structres and come with metadata like headers and indexes so we must remove them to run our model and store the numeric values.\n",
    "\n",
    "Key use rule: Users must ensure that when selecting feautres, utilise single brackets [] to ensure data is pandas series and not dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ebb8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorise(x):\n",
    "    if isinstance(x, (pd.DataFrame)):\n",
    "        print(\"Error: Select features to create pandas series\")\n",
    "    elif isinstance(x, (tuple, list, np.ndarray)): \n",
    "        x = list(x)\n",
    "        print(\"Ready to go!\")\n",
    "    elif isinstance(x, (pd.Series)):\n",
    "        x = list(x.values)\n",
    "        print(\"Ready to go!\")\n",
    "    else:\n",
    "        print(\"Error: Please insert iteratable, numeric type like `tuple` or `pd.Series`\")\n",
    "    return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f97fc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df['Weight']\n",
    "df3 = df[['Weight']]\n",
    "print(df2)\n",
    "print(df3)\n",
    "print(type(df2))        #converts the dataframe into a series\n",
    "print(type(df3))        #keeps dataframe as dataframe, bad since it keeps and index \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa5527c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df2.values #returns values of the series so series is correct type\n",
    "df3.values #returns multi-dimensional array that is not good for linear regression since we want two n x1 feautures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c706d9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorise(df2) #testing on valid input \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e609f250",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorise(df3) #testing error message of pandas dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c808504f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 64\n",
    "vectorise(x) #testing on other types "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a53d8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class  LinearRegression(object):\n",
    "    def __init__(self, learning_rate, epochs, weight=0, bias=0, ): #initialises the attributes of the class at 0\n",
    "        self.weight = weight              #stores weight \n",
    "        self.bias = bias                  #stores bias \n",
    "        self.x = []                       #creates empty list to store our predictor variables (x)  \n",
    "        self.y = []                       #creates empty list to store our our predicted variables (y) \n",
    "        self.learning_rate = learning_rate #setting your own learning rate \n",
    "        self.epochs = int(epochs)               #setting your own number of epochs \n",
    " \n",
    "    def vectorise(self, x, y):             #defining method to store the data points to be modelled\n",
    "        self.x = x                  #storing the values of x (independent variable) within the class \n",
    "        self.y = y                  #storing the values of y (dependent variable) within the class \n",
    "\n",
    "    def predict_y (self):                 #calculating the predicted y[i] for our optimisation later \n",
    "        y_predict =[]                     #creating an empty list to store all predicted y values \n",
    "        n = len(self.y)                   #range that we iterate over (number of values of y)\n",
    "\n",
    "        for i in range(n):                #looping over the number of values we have in the dataset \n",
    "            y_predict.append(self.weight*self.x[i] + self.bias)     #calculating predicted y values with line equation and adding predicted values to our list \n",
    "        return y_predict                  \n",
    "\n",
    "#NUMERICAL OPTIMISATION \n",
    "#Creating method to get weight\n",
    "    def partial_w(self):                  #partial derivative in regard to weight \n",
    "        y_predict = self.predict_y()      #predicted y value is equal to calling the internal method we defined above \n",
    "        gradient = 0 \n",
    "        n=len(self.y)\n",
    "\n",
    "        for i in range(n):\n",
    "            gradient += self.x[i]*(y_predict[i] - self.y[i])         #partial derivative equation to calculate total partial derivative of weight in regards to error function\n",
    "        return (-2/n)*gradient                                       #returns the  weight eqaution that minimises the partial derivative in regard to error function\n",
    "\n",
    "#Creating method to get bias \n",
    "    def partial_b(self):\n",
    "        y_predict = self.predict_y()\n",
    "        gradient = 0\n",
    "        n=len(self.y)\n",
    "\n",
    "        for i in range(n):\n",
    "            gradient += (y_predict[i]- self.y[i])                     #partial derivative equation to calculate total partial derivative of bias in regards to error function\n",
    "        return (-2/n)*gradient                                        #returns the  bias equation that minimises the partial derivative in regard to error function\n",
    "\n",
    "#Gradient Descent - iterating over multiple steps with our partial weight and bias functions \n",
    "    def optimise(self): \n",
    "        learn_rate = self.learning_rate                #size of steps we make \"downhill\" to minimise total error in regards to the weight and bias \n",
    "\n",
    "        for i in range(self.epochs):             #number of \"epochs\"/ steps we take in order to minimise aggregate error \n",
    "            self.weight = self.weight + learn_rate * self.partial_w() #optimised weight by calling partial_w 10000 times\n",
    "            self.bias = self.bias + learn_rate * self.partial_b()     #optimised bias  by calling partial_b 10000 times\n",
    "            if i % 10 == 0:                #prints out the weight and bias every 10 epochs \n",
    "                print(self.weight, self.bias)\n",
    "    \n",
    "#Residuals - creating a new residuals method to display deviation of predicted values from actual values\n",
    "    def residuals(self):\n",
    "        residuals = []\n",
    "        n=len(self.x) \n",
    "\n",
    "        for i in range(n):\n",
    "            residuals.append(self.y[i] - (self.weight * self.x[i] + self.bias)) #adding to the list called \"residuals\" the difference between actual and predicted y\n",
    "        return residuals                                                        \n",
    "\n",
    "#EVALUATION METRICS  -  these are key values that we will use to quantify how good our model predicts the data it is trained on. \n",
    "#Mean Square Error (MSE)  - the average squared deviation from actual values of y\n",
    "\n",
    "    def mse(self):\n",
    "        mse = 0                              #initialising our mse as a variable  that will be updated through the loops \n",
    "        mse_list = []                        #empty list to store our mse\n",
    "        n=len(self.y)                        #creating length for range to iterate over\n",
    "        total_error = []                     #what is the total error i.e actual - predicted y\n",
    "        self.square_error = 0                #stores the square error of the deviations \n",
    "\n",
    "        for i in range(n):                   #iterating to calculate the mse \n",
    "            total_error.append((self.y[i] - (self.weight * self.x[i] + self.bias))**2) #deviation from actual y ^2\n",
    "            self.square_error += total_error[i] #storing square error to be used in future calculations\n",
    "            mse_list.append((1/n)*total_error[i]) #storing mse in the list using the mean squared error formula \n",
    "            mse += mse_list[i]               #iterates by adding all elements in the list together to give us our aggreagte mse\n",
    "        return mse \n",
    "    \n",
    "#R^2 -  how much of the deviation in y is explained by our model\n",
    "    def rsquared(self):\n",
    "        n=len(self.y)\n",
    "        self.avg_y = 0                       #initial value of the average of our actual y values \n",
    "\n",
    "    #Average y- average of our actual y    \n",
    "        for i in range(n):\n",
    "            self.avg_y += ((1/n)*self.y[i])  #calculating the average value of actual y \n",
    "        \n",
    "    #Total sum of squares -  \n",
    "        self.sum_squares = 0                 #creating an object called sum_squares to be used further in the function \n",
    "        sum_squares_list =[]                 #empty list to store values of sum of squares \n",
    "        n = len(self.y)\n",
    "\n",
    "        for i in range(n):\n",
    "            sum_squares_list.append((self.y[i] - self.avg_y)**2) #the squared values of actual - predicted y  and storing them in the empty list above \n",
    "            self.sum_squares += sum_squares_list[i]              #adding togther all of the sum of squares into initial variable sum_squares \n",
    "\n",
    "    #Final calculation \n",
    "        rsquared = 0                         #initialising our value of rsquared as 0 \n",
    "        n=len(self.y)\n",
    "\n",
    "        rsquared = (1-(self.square_error/self.sum_squares)) #calculating R^2 with our instances of sum of squares and square error \n",
    "        return rsquared \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d72b8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Weight':[136,140,155,180], \n",
    "                  'Age':[20,23,27,31]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b11113",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb10d51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2=df[['Weight']]\n",
    "print(x_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5649e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2=df[['Age']]\n",
    "print(y_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4df6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 =LinearRegression(0.05, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d36b9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.vectorise(x_2,y_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f8fcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.optimise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2c8620",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 30"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (linreg)",
   "language": "python",
   "name": "linreg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
