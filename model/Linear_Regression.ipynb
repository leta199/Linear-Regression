{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3ec2daf",
   "metadata": {},
   "source": [
    "# LINEAR REGRESSION "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2488c407",
   "metadata": {},
   "source": [
    "## CHAPTER #1 - LINEAR REGRESSION MODEL "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923bad8d",
   "metadata": {},
   "source": [
    " ### CLASS CREATION  \n",
    " This class will contain all of the model logic.   \n",
    " The class contains the Linear regression model itself. \n",
    " Each object is a \"best fit line\"  i.e a line for each set of training data.   \n",
    "\n",
    " The attributes contain the:  \n",
    " - weight (coefficeint of the independent variable, x)  \n",
    " - bias (y-intercept) of the line \n",
    "\n",
    " The methods contain all the \"verbs\" of the model (functions that change the weight and bias), namely:  \n",
    " - how the model learns the weight and bias. \n",
    " - how the model fits the line to the data.  \n",
    " - how the line is evaluated e.g R^2 and RMSE. \n",
    " - how close our predictions are i.e residuals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdeb379",
   "metadata": {},
   "outputs": [],
   "source": [
    "class  LinearRegression(object):\n",
    "    def __init__(self, weight=0, bias=0): #initialises the attributes of the class at 0\n",
    "        self.weight = weight              #stores weight \n",
    "        self.bias = bias                  #stores bias \n",
    "        self.x = []                       #creates empty list to store our predictor variables (x)  \n",
    "        self.y = []                       #creates empty list to store our our predicted variables (y) \n",
    " \n",
    "    def vectorise(self, x,y):             #defining method to store the data points to be modelled\n",
    "        self.x = x                        #storing the values of x (independent variable) within the class \n",
    "        self.y = y                        #storing the values of y (dependent variable) within the class \n",
    "\n",
    "    def predict_y (self):                 #calculating the predicted y[i] for our optimisation later \n",
    "        y_predict =[]                     #creating an empty list to store all predicted y values \n",
    "        n = len(self.y)                   #range that we iterate over (number of values of y)\n",
    "\n",
    "        for i in range(n):                #looping over the number of values we have in the dataset \n",
    "            y_predict.append(self.weight*self.x[i] + self.bias)     #calculating predicted y values with line equation and adding predicted values to our list \n",
    "        return y_predict                  \n",
    "\n",
    "#NUMERICAL OPTIMISATION \n",
    "#Creating method to get weight\n",
    "    def partial_w(self):                  #partial derivative in regard to weight \n",
    "        y_predict = self.predict_y()      #predicted y value is equal to calling the internal method we defined above \n",
    "        gradient = 0 \n",
    "        n=len(self.y)\n",
    "\n",
    "        for i in range(n):\n",
    "            gradient += self.x[i]*(y_predict[i] - self.y[i])         #partial derivative equation to calculate total partial derivative of weight in regards to error function\n",
    "        return (-2/n)*gradient                                       #returns the  weight eqaution that minimises the partial derivative in regard to error function\n",
    "\n",
    "#Creating method to get bias \n",
    "    def partial_b(self):\n",
    "        y_predict = self.predict_y()\n",
    "        gradient = 0\n",
    "        n=len(self.y)\n",
    "\n",
    "        for i in range(n):\n",
    "            gradient += (y_predict[i]- self.y[i])                     #partial derivative equation to calculate total partial derivative of bias in regards to error function\n",
    "        return (-2/n)*gradient                                        #returns the  bias equation that minimises the partial derivative in regard to error function\n",
    "\n",
    "#Gradient Descent - iterating over multiple steps with our partial weight and bias functions \n",
    "    def optimise(self): \n",
    "        learn_rate = 0.005                 #size of steps we make \"downhill\" to minimise total error in regards to the weight and bias \n",
    "\n",
    "        for i in range(10000):             #number of \"epochs\"/ steps we take in order to minimise aggregate error \n",
    "            self.weight = self.weight + learn_rate * self.partial_w() #optimised weight by calling partial_w 10000 times\n",
    "            self.bias = self.bias + learn_rate * self.partial_b()     #optimised bias  by calling partial_b 10000 times\n",
    "            if i % 10 == 0:                #prints out the weight and bias every 10 epochs \n",
    "                print(self.weight, self.bias)\n",
    "    \n",
    "#Residuals - creating a new residuals method to display deviation of predicted values from actual values\n",
    "    def residuals(self):\n",
    "        residuals = []\n",
    "        n=len(self.x) \n",
    "\n",
    "        for i in range(n):\n",
    "            residuals.append(self.y[i] - (self.weight * self.x[i] + self.bias)) #adding to the list called \"residuals\" the difference between actual and predicted y\n",
    "        return residuals                                                        \n",
    "\n",
    "#EVALUATION METRICS  -  these are key values that we will use to quantify how good our model predicts the data it is trained on. \n",
    "#Mean Square Error (MSE)  - the average squared deviation from actual values of y\n",
    "\n",
    "    def mse(self):\n",
    "        mse = 0                              #initialising our mse as a variable  that will be updated through the loops \n",
    "        mse_list = []                        #empty list to store our mse\n",
    "        n=len(self.y)                        #creating length for range to iterate over\n",
    "        total_error = []                     #what is the total error i.e actual - predicted y\n",
    "        self.square_error = 0                #stores the square error of the deviations \n",
    "\n",
    "        for i in range(n):                   #iterating to calculate the mse \n",
    "            total_error.append((self.y[i] - (self.weight * self.x[i] + self.bias))**2) #deviation from actual y ^2\n",
    "            self.square_error += total_error[i] #storing square error to be used in future calculations\n",
    "            mse_list.append((1/n)*total_error[i]) #storing mse in the list using the mean squared error formula \n",
    "            mse += mse_list[i]               #iterates by adding all elements in the list together to give us our aggreagte mse\n",
    "        return mse \n",
    "    \n",
    "#R^2 -  how much of the deviation in y is explained by our model\n",
    "    def rsquared(self):\n",
    "        n=len(self.y)\n",
    "        self.avg_y = 0                       #initial value of the average of our actual y values \n",
    "\n",
    "    #Average y- average of our actual y    \n",
    "        for i in range(n):\n",
    "            self.avg_y += ((1/n)*self.y[i])  #calculating the average value of actual y \n",
    "        \n",
    "    #Total sum of squares -  \n",
    "        self.sum_squares = 0                 #creating an object called sum_squares to be used further in the function \n",
    "        sum_squares_list =[]                 #empty list to store values of sum of squares \n",
    "        n = len(self.y)\n",
    "\n",
    "        for i in range(n):\n",
    "            sum_squares_list.append((self.y[i] - self.avg_y)**2) #the squared values of actual - predicted y  and storing them in the empty list above \n",
    "            self.sum_squares += sum_squares_list[i]              #adding togther all of the sum of squares into initial variable sum_squares \n",
    "\n",
    "    #Final calculation \n",
    "        rsquared = 0                         #initialising our value of rsquared as 0 \n",
    "        n=len(self.y)\n",
    "\n",
    "        rsquared = (1-(self.square_error/self.sum_squares)) #calculating R^2 with our instances of sum of squares and square error \n",
    "        return rsquared \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244a6b2b",
   "metadata": {},
   "source": [
    "EXAMPLE WITH SIMPLE LISTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd657916",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1,2,3,4,5]\n",
    "y = [6,7,8,9,10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2850f30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install matplotlib\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a664c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965ff661",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866e5ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.vectorise(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e83ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953ad096",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_y()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd4758d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.residuals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa3d6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.mse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d6dd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.rsquared()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (linreg)",
   "language": "python",
   "name": "linreg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
