{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3ec2daf",
   "metadata": {},
   "source": [
    "# LINEAR REGRESSION "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2488c407",
   "metadata": {},
   "source": [
    "## CHAPTER #1 - LINEAR REGRESSION MODEL "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923bad8d",
   "metadata": {},
   "source": [
    " ### CLASS CREATION  \n",
    " This class will contain all of the model logic.   \n",
    " The class contains the Linear regression model itself. \n",
    " Each object is a \"best fit line\"  i.e a line for each set of training data.   \n",
    "\n",
    " The attributes contain the:  \n",
    " - weight (coefficeint of the independent variable, x)  \n",
    " - bias (y-intercept) of the line \n",
    "\n",
    " The methods contain all the \"verbs\" of the model (functions that change the weight and bias), namely:  \n",
    " - how the model learns the weight and bias. \n",
    " - how the model fits the line to the data.  \n",
    " - how the line is evaluated e.g R^2 and RMSE. \n",
    " - how close our predictions are i.e residuals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdeb379",
   "metadata": {},
   "outputs": [],
   "source": [
    "class  LinearRegressionOld(object):\n",
    "    def __init__(self, learning_rate, epochs, weight=0, bias=0, ): #initialises the attributes of the class at 0\n",
    "        self.weight = weight              #stores weight \n",
    "        self.bias = bias                  #stores bias \n",
    "        self.x = []                       #creates empty list to store our predictor variables (x)  \n",
    "        self.y = []                       #creates empty list to store our our predicted variables (y) \n",
    "        self.learning_rate = learning_rate #setting your own learning rate \n",
    "        self.epochs = int(epochs )              #setting your own number of epochs \n",
    " \n",
    "    def vectorise(self, x,y):             #defining method to store the data points to be modelled\n",
    "        self.x = x                  #storing the values of x (independent variable) within the class \n",
    "        self.y = y                  #storing the values of y (dependent variable) within the class \n",
    "\n",
    "    def predict_y (self):                 #calculating the predicted y[i] for our optimisation later \n",
    "        y_predict =[]                     #creating an empty list to store all predicted y values \n",
    "        n = len(self.y)                   #range that we iterate over (number of values of y)\n",
    "\n",
    "        for i in range(n):                #looping over the number of values we have in the dataset \n",
    "            y_predict.append(self.weight*self.x[i] + self.bias)     #calculating predicted y values with line equation and adding predicted values to our list \n",
    "        return y_predict                  \n",
    "\n",
    "#NUMERICAL OPTIMISATION \n",
    "#Creating method to get weight\n",
    "    def partial_w(self):                  #partial derivative in regard to weight \n",
    "        y_predict = self.predict_y()      #predicted y value is equal to calling the internal method we defined above \n",
    "        gradient = 0 \n",
    "        n=len(self.y)\n",
    "\n",
    "        for i in range(n):\n",
    "            gradient += self.x[i]*(y_predict[i] - self.y[i])         #partial derivative equation to calculate total partial derivative of weight in regards to error function\n",
    "        return (-2/n)*gradient                                       #returns the  weight eqaution that minimises the partial derivative in regard to error function\n",
    "\n",
    "#Creating method to get bias \n",
    "    def partial_b(self):\n",
    "        y_predict = self.predict_y()\n",
    "        gradient = 0\n",
    "        n=len(self.y)\n",
    "\n",
    "        for i in range(n):\n",
    "            gradient += (y_predict[i]- self.y[i])                     #partial derivative equation to calculate total partial derivative of bias in regards to error function\n",
    "        return (-2/n)*gradient                                        #returns the  bias equation that minimises the partial derivative in regard to error function\n",
    "\n",
    "#Gradient Descent - iterating over multiple steps with our partial weight and bias functions \n",
    "    def optimise(self): \n",
    "        learn_rate = self.learning_rate                #size of steps we make \"downhill\" to minimise total error in regards to the weight and bias \n",
    "\n",
    "        for i in range(self.epochs):             #number of \"epochs\"/ steps we take in order to minimise aggregate error \n",
    "            self.weight = self.weight + learn_rate * self.partial_w() #optimised weight by calling partial_w 10000 times\n",
    "            self.bias = self.bias + learn_rate * self.partial_b()     #optimised bias  by calling partial_b 10000 times\n",
    "            if i % 10 == 0:                #prints out the weight and bias every 10 epochs \n",
    "                print(self.weight, self.bias)\n",
    "    \n",
    "#Residuals - creating a new residuals method to display deviation of predicted values from actual values\n",
    "    def residuals(self):\n",
    "        residuals = []\n",
    "        n=len(self.x) \n",
    "\n",
    "        for i in range(n):\n",
    "            residuals.append(self.y[i] - (self.weight * self.x[i] + self.bias)) #adding to the list called \"residuals\" the difference between actual and predicted y\n",
    "        return residuals                                                        \n",
    "\n",
    "#EVALUATION METRICS  -  these are key values that we will use to quantify how good our model predicts the data it is trained on. \n",
    "#Mean Square Error (MSE)  - the average squared deviation from actual values of y\n",
    "\n",
    "    def mse(self):\n",
    "        mse = 0                              #initialising our mse as a variable  that will be updated through the loops \n",
    "        mse_list = []                        #empty list to store our mse\n",
    "        n=len(self.y)                        #creating length for range to iterate over\n",
    "        total_error = []                     #what is the total error i.e actual - predicted y\n",
    "        self.square_error = 0                #stores the square error of the deviations \n",
    "\n",
    "        for i in range(n):                   #iterating to calculate the mse \n",
    "            total_error.append((self.y[i] - (self.weight * self.x[i] + self.bias))**2) #deviation from actual y ^2\n",
    "            self.square_error += total_error[i] #storing square error to be used in future calculations\n",
    "            mse_list.append((1/n)*total_error[i]) #storing mse in the list using the mean squared error formula \n",
    "            mse += mse_list[i]               #iterates by adding all elements in the list together to give us our aggreagte mse\n",
    "        return mse \n",
    "    \n",
    "#R^2 -  how much of the deviation in y is explained by our model\n",
    "    def rsquared(self):\n",
    "        n=len(self.y)\n",
    "        self.avg_y = 0                       #initial value of the average of our actual y values \n",
    "\n",
    "    #Average y- average of our actual y    \n",
    "        for i in range(n):\n",
    "            self.avg_y += ((1/n)*self.y[i])  #calculating the average value of actual y \n",
    "        \n",
    "    #Total sum of squares -  \n",
    "        self.sum_squares = 0                 #creating an object called sum_squares to be used further in the function \n",
    "        sum_squares_list =[]                 #empty list to store values of sum of squares \n",
    "        n = len(self.y)\n",
    "\n",
    "        for i in range(n):\n",
    "            sum_squares_list.append((self.y[i] - self.avg_y)**2) #the squared values of actual - predicted y  and storing them in the empty list above \n",
    "            self.sum_squares += sum_squares_list[i]              #adding togther all of the sum of squares into initial variable sum_squares \n",
    "\n",
    "    #Final calculation \n",
    "        rsquared = 0                         #initialising our value of rsquared as 0 \n",
    "        n=len(self.y)\n",
    "\n",
    "        rsquared = (1-(self.square_error/self.sum_squares)) #calculating R^2 with our instances of sum of squares and square error \n",
    "        return rsquared \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244a6b2b",
   "metadata": {},
   "source": [
    "EXAMPLE WITH SIMPLE LISTS\n",
    "Now I have added arguments for adjustable learning rates and epochs numbers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd657916",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = (1,2,3,4,5)\n",
    "y = (6,7,8,9,10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2850f30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install matplotlib\n",
    "%pip install pandas\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a664c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965ff661",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegressionOld(0.05, 10000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866e5ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.vectorise(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e83ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953ad096",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_y()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd4758d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.residuals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa3d6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.mse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d6dd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.rsquared()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f69169",
   "metadata": {},
   "source": [
    "## IMPLEMENTATION WITH DIIFFERENT TYPES\n",
    "Upon further inspection I realised that a big problem that would reveal itself is that when gradient descent runs, different types of  x and y values can make the model crash.  \n",
    "\n",
    "For example, data in vectorise() taken as dataframes cannot work as the dimensions are incorrect and there are headers and indexes. Therefore I need a method to normalise all of the input of dependent and independt variables i.e turn them all into iterable, indexable and numerical compund types like lists.\n",
    "\n",
    "Thankfully, that is what my vectorise() method is for. I just need to modify it so that:  \n",
    "1) It takes in the values we want to model whether they be pandas series, lists, tuples etc\n",
    "2) It checks what types our inputs are coming in as. \n",
    "   -  If they are iterable and only numeric e.g lists, tuples, numpy arrays - great \n",
    "   -  If not iterable or not only numeric - not great and needs to be wrapped into iterable like a list and made only numeric\n",
    "3) Ensure data in itrable like lists is numeric and not strings \n",
    "4) Store these iterables  as self.x and self.y "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fc529a",
   "metadata": {},
   "source": [
    "## HOW DO WE CHECK TYPES OF INPUTS?\n",
    "\n",
    "We require iterables of the type that can:\n",
    "1) Can be indexed\n",
    "2) Are numeric   \n",
    "- This leaves us with valid data types of **lists, tuples, numpy arrays and pandas series**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d33feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340f1281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectoriseold(x):\n",
    "    if isinstance(x, (tuple, list, np.ndarray, pd.Series, pd.DataFrame)):\n",
    "        return \"Ready to go!\"\n",
    "    else:\n",
    "        return \"Just a moment\"\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da36fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = (10,20,30) #Trial with a tuple\n",
    "vectoriseold(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e80a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [30,40,50] #Trial with a list\n",
    "vectoriseold(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac2a53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Weight':[136.3,140.5,155.2,180.0], #Trial with a dataframe \n",
    "                  'Age':[20,23,27,31]}) \n",
    "vectoriseold(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d65889",
   "metadata": {},
   "source": [
    "## CONVERTING ITERABLES INTO LISTS \n",
    "Now that we can check if our input is an interable like we want, let us convert them into common types.\n",
    "I will use Python base types of a list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d6f5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorisenewer(x):\n",
    "    if isinstance(x, (tuple, list, np.ndarray, pd.Series)): #removed pd.Dataframe since our linear regression handles n x 1 features so dataframes are incorrect dimension.\n",
    "        x = list(x)\n",
    "        return \"Ready to go!\"\n",
    "    else:\n",
    "        print(\"Just a moment\")\n",
    "        x = [x]\n",
    "        print(\"Ready to go!\")\n",
    "    return x\n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67575a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[['Weight']]\n",
    "print(df1)\n",
    "vectorisenewer(df1) #works but returns entire dataframe as list with heading and index. \n",
    "                    #need to remove all metadata from the pandas dataframe \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62502db9",
   "metadata": {},
   "source": [
    "## DEALING WITH PANDAS SERIES AND DATAFRAMES\n",
    "So far our iterables are always just numeric values that can easily be converted into lists.\n",
    "Pandas dataframes are different. They are lablled data structres and come with metadata like headers and indexes so we must remove them to run our model and store the numeric values.\n",
    "\n",
    "Key use rule: Users must ensure that when selecting feautres, utilise single brackets [] to ensure data is pandas series and not dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ebb8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorise(x):\n",
    "    if isinstance(x, (pd.DataFrame)):\n",
    "        print(\"Error: Select features to create pandas series\")\n",
    "    elif isinstance(x, (tuple, list, np.ndarray)): \n",
    "        x = list(x)\n",
    "        print(\"Ready to go!\")\n",
    "    elif isinstance(x, (pd.Series)):\n",
    "        x = list(x.values)\n",
    "        print(\"Ready to go!\")\n",
    "    else:\n",
    "        print(\"Error: Please insert iteratable, numeric type like `tuple` or `pd.Series`\")\n",
    "    return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f97fc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df['Weight']\n",
    "df3 = df[['Weight']]\n",
    "print(df2)\n",
    "print(df3)\n",
    "print(type(df2))        #converts the dataframe into a series\n",
    "print(type(df3))        #keeps dataframe as dataframe, bad since it keeps and index \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa5527c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df2.values #returns values of the series so series is correct type\n",
    "df3.values #returns multi-dimensional array that is not good for linear regression since we want two n x1 feautures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c706d9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorise(df2) #testing on valid input \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e609f250",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorise(df3) #testing error message of pandas dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c808504f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 64\n",
    "vectorise(x) #testing on other types "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a53d8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class  LinearRegression_newer(object):\n",
    "    def __init__(self, learning_rate, epochs, weight=0, bias=0, ): #initialises the attributes of the class at with adjustable learning rate and epochs \n",
    "        self.weight = weight              #stores weight \n",
    "        self.bias = bias                  #stores bias \n",
    "        self.x = []                       #creates empty list to store our predictor variables (x)  \n",
    "        self.y = []                       #creates empty list to store our our predicted variables (y) \n",
    "        self.learning_rate = learning_rate #setting your own learning rate \n",
    "        self.epochs = int(epochs)               #setting your own number of epochs \n",
    " \n",
    "    def vectorise_x(self, x):                     #method to store, convert and display error messages for input data \n",
    "        if isinstance(x, (pd.DataFrame)):       #if input data is as dataframe display error message to request series \n",
    "            print(\"Error: Select features to create pandas series\")\n",
    "        elif isinstance(x, (tuple, list, np.ndarray)): #valid input data as the type of tuples, lists, arrays that are numeric, iterable and indexable \n",
    "            self.x = list(x)                           #convert these valid types into lists \n",
    "            print(\"Ready to go!\")\n",
    "        elif isinstance(x, (pd.Series)):               #method to handle panda series \n",
    "            self.x = list(x.values)                    #extract the values from the pandas series \n",
    "            print(\"Ready to go!\")\n",
    "        else:\n",
    "            print(\"Error: Please insert iteratable, numeric type like `tuple` or `pd.Series`\") #any other data type displays error message \n",
    "        return self.x\n",
    "                      \n",
    "    def vectorise_y(self, y):\n",
    "        if isinstance(y, (pd.DataFrame)):\n",
    "            print(\"Error: Select features to create pandas series\")\n",
    "        elif isinstance(y, (tuple, list, np.ndarray)): \n",
    "            self.y = list(y)\n",
    "            print(\"Ready to go!\")\n",
    "        elif isinstance(y, (pd.Series)):\n",
    "            self.y = list(y.values)\n",
    "            print(\"Ready to go!\")\n",
    "        else:\n",
    "            print(\"Error: Please insert iteratable, numeric type like `tuple` or `pd.Series`\")\n",
    "        return self.y\n",
    "\n",
    "    def predict_y (self):                 #calculating the predicted y[i] for our optimisation later \n",
    "        y_predict =[]                     #creating an empty list to store all predicted y values \n",
    "        n = len(self.y)                   #range that we iterate over (number of values of y)\n",
    "\n",
    "        for i in range(n):                #looping over the number of values we have in the dataset \n",
    "            y_predict.append(self.weight*self.x[i] + self.bias)     #calculating predicted y values with line equation and adding predicted values to our list \n",
    "        return y_predict                  \n",
    "\n",
    "#NUMERICAL OPTIMISATION \n",
    "#Creating method to get weight\n",
    "    def partial_w(self):                  #partial derivative in regard to weight \n",
    "        y_predict = self.predict_y()      #predicted y value is equal to calling the internal method we defined above \n",
    "        gradient = 0 \n",
    "        n=len(self.y)\n",
    "\n",
    "        for i in range(n):\n",
    "            gradient += self.x[i]*(y_predict[i] - self.y[i])         #partial derivative equation to calculate total partial derivative of weight in regards to error function\n",
    "        return (-2/n)*gradient                                       #returns the  weight eqaution that minimises the partial derivative in regard to error function\n",
    "\n",
    "#Creating method to get bias \n",
    "    def partial_b(self):\n",
    "        y_predict = self.predict_y()\n",
    "        gradient = 0\n",
    "        n=len(self.y)\n",
    "\n",
    "        for i in range(n):\n",
    "            gradient += (y_predict[i]- self.y[i])                     #partial derivative equation to calculate total partial derivative of bias in regards to error function\n",
    "        return (-2/n)*gradient                                        #returns the  bias equation that minimises the partial derivative in regard to error function\n",
    "\n",
    "#Gradient Descent - iterating over multiple steps with our partial weight and bias functions \n",
    "    def optimise(self): \n",
    "        learn_rate = self.learning_rate                #size of steps we make \"downhill\" to minimise total error in regards to the weight and bias \n",
    "\n",
    "        for i in range(self.epochs):             #number of \"epochs\"/ steps we take in order to minimise aggregate error \n",
    "            self.weight = self.weight + learn_rate * self.partial_w() #optimised weight by calling partial_w 10000 times\n",
    "            self.bias = self.bias + learn_rate * self.partial_b()     #optimised bias  by calling partial_b 10000 times\n",
    "            if i % 10 == 0:                #prints out the weight and bias every 10 epochs \n",
    "                print(self.weight, self.bias)\n",
    "    \n",
    "#Residuals - creating a new residuals method to display deviation of predicted values from actual values\n",
    "    def residuals(self):\n",
    "        residuals = []\n",
    "        n=len(self.x) \n",
    "\n",
    "        for i in range(n):\n",
    "            residuals.append(self.y[i] - (self.weight * self.x[i] + self.bias)) #adding to the list called \"residuals\" the difference between actual and predicted y\n",
    "        return residuals                                                        \n",
    "\n",
    "#EVALUATION METRICS  -  these are key values that we will use to quantify how good our model predicts the data it is trained on. \n",
    "#Mean Square Error (MSE)  - the average squared deviation from actual values of y\n",
    "\n",
    "    def mse(self):\n",
    "        mse = 0                              #initialising our mse as a variable  that will be updated through the loops \n",
    "        mse_list = []                        #empty list to store our mse\n",
    "        n=len(self.y)                        #creating length for range to iterate over\n",
    "        total_error = []                     #what is the total error i.e actual - predicted y\n",
    "        self.square_error = 0                #stores the square error of the deviations \n",
    "\n",
    "        for i in range(n):                   #iterating to calculate the mse \n",
    "            total_error.append((self.y[i] - (self.weight * self.x[i] + self.bias))**2) #deviation from actual y ^2\n",
    "            self.square_error += total_error[i] #storing square error to be used in future calculations\n",
    "            mse_list.append((1/n)*total_error[i]) #storing mse in the list using the mean squared error formula \n",
    "            mse += mse_list[i]               #iterates by adding all elements in the list together to give us our aggreagte mse\n",
    "        return mse \n",
    "    \n",
    "#R^2 -  how much of the deviation in y is explained by our model\n",
    "    def rsquared(self):\n",
    "        n=len(self.y)\n",
    "        self.avg_y = 0                       #initial value of the average of our actual y values \n",
    "\n",
    "    #Average y- average of our actual y    \n",
    "        for i in range(n):\n",
    "            self.avg_y += ((1/n)*self.y[i])  #calculating the average value of actual y \n",
    "        \n",
    "    #Total sum of squares -  \n",
    "        self.sum_squares = 0                 #creating an object called sum_squares to be used further in the function \n",
    "        sum_squares_list =[]                 #empty list to store values of sum of squares \n",
    "        n = len(self.y)\n",
    "\n",
    "        for i in range(n):\n",
    "            sum_squares_list.append((self.y[i] - self.avg_y)**2) #the squared values of actual - predicted y  and storing them in the empty list above \n",
    "            self.sum_squares += sum_squares_list[i]              #adding togther all of the sum of squares into initial variable sum_squares \n",
    "\n",
    "    #Final calculation \n",
    "        rsquared = 0                         #initialising our value of rsquared as 0 \n",
    "        n=len(self.y)\n",
    "\n",
    "        rsquared = (1-(self.square_error/self.sum_squares)) #calculating R^2 with our instances of sum of squares and square error \n",
    "        return rsquared \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79a72ff",
   "metadata": {},
   "source": [
    "### PRACTICE WITH DATAFRAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d72b8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Weight':[136,140,155,180], \n",
    "                  'Age':[20,23,27,31]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b11113",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb10d51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2=df['Weight']\n",
    "print(x_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5649e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2=df['Age']\n",
    "print(y_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb4eb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_2, y_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4df6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 =LinearRegression_newer(0.0000005, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d36b9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.vectorise_x(x_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f8fcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.vectorise_y(y_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2c8620",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.optimise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c157ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.predict_y()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dfcefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.residuals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727f9da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.mse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643b4bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.rsquared()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28692ab8",
   "metadata": {},
   "source": [
    "### QUALITY OF LIFE IMPORVEMENTS \n",
    "I just want to make the whole class a little bit more clean and able to display output in a better way. \n",
    "This includes: \n",
    "1) Make R^2 and mse indepdnent of each other. \n",
    "2) Make output neater insetad of \"np.float\" just the raw values\n",
    "3) Display the inner working of the model such as predicted y values \n",
    "4) Overall a little more efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e70543",
   "metadata": {},
   "outputs": [],
   "source": [
    "class  LinearRegression(object):\n",
    "    def __init__(self, learning_rate, epochs, weight=0, bias=0, ): #initialises the attributes of the class at with adjustable learning rate and epochs \n",
    "        self.weight = weight              #stores weight \n",
    "        self.bias = bias                  #stores bias \n",
    "        self.x = []                       #creates empty list to store our predictor variables (x)  \n",
    "        self.y = []                       #creates empty list to store our our predicted variables (y) \n",
    "        self.learning_rate = learning_rate #setting your own learning rate \n",
    "        self.epochs = int(epochs)               #setting your own number of epochs \n",
    " \n",
    "    def vectorise_x(self, x):                   #method to store, convert and display error messages for input data \n",
    "        if isinstance(x, (pd.DataFrame)):       #if input data is as dataframe display error message to request series \n",
    "            print(\"Error: Select features to create pandas series\")\n",
    "        elif isinstance(x, (tuple, list, np.ndarray)): #valid input data as the type of tuples, lists, arrays that are numeric, iterable and indexable \n",
    "            self.x = list(x)                           #convert these valid types into lists \n",
    "            print(\"Ready to go!\")\n",
    "        elif isinstance(x, (pd.Series)):               #method to handle panda series \n",
    "            self.x = list(x.values)                    #extract the values from the pandas series \n",
    "            print(\"Ready to go!\")\n",
    "        else:\n",
    "            print(\"Error: Please insert iteratable, numeric type like `tuple` or `pd.Series`\") #any other data type displays error message \n",
    "                      \n",
    "    def vectorise_y(self, y):\n",
    "        if isinstance(y, (pd.DataFrame)):\n",
    "            print(\"Error: Select features to create pandas series\")\n",
    "        elif isinstance(y, (tuple, list, np.ndarray)): \n",
    "            self.y = list(y)\n",
    "            print(\"Ready to go!\")\n",
    "        elif isinstance(y, (pd.Series)):\n",
    "            self.y = list(y.values)\n",
    "            print(\"Ready to go!\")\n",
    "        else:\n",
    "            print(\"Error: Please insert iteratable, numeric type like `tuple` or `pd.Series`\")\n",
    "        \n",
    "                      \n",
    "\n",
    "    def predict_y (self):                 #calculating the predicted y[i] for our optimisation later \n",
    "        self.y_predict =[]                #creating an empty list to store all predicted y values \n",
    "        n = len(self.y)                   #range that we iterate over (number of values of y)\n",
    "\n",
    "        for i in range(n):                #looping over the number of values we have in the dataset \n",
    "            self.y_predict.append(self.weight*self.x[i] + self.bias)     #calculating predicted y values with line equation and adding predicted values to our list \n",
    "        return self.y_predict                             \n",
    "\n",
    "#NUMERICAL OPTIMISATION \n",
    "#Creating method to get weight\n",
    "    def partial_w(self):                       #partial derivative in regard to weight \n",
    "        self.y_predict = self.predict_y()      #predicted y value is equal to calling the internal method we defined above \n",
    "        gradient = 0 \n",
    "        n=len(self.y)\n",
    "\n",
    "        for i in range(n):\n",
    "            gradient += self.x[i]*(self.y_predict[i] - self.y[i])    #partial derivative equation to calculate total partial derivative of weight in regards to error function\n",
    "        return (-2/n)*gradient                                       #returns the  weight eqaution that minimises the partial derivative in regard to error function\n",
    "\n",
    "#Creating method to get bias \n",
    "    def partial_b(self):\n",
    "       n=len(self.y)\n",
    "       gradient = 0\n",
    "       self.y_predict = self.predict_y()\n",
    "       \n",
    "       for i in range(n):\n",
    "            gradient += (self.y_predict[i]- self.y[i])               #partial derivative equation to calculate total partial derivative of bias in regards to error function\n",
    "       return (-2/n)*gradient                                        #returns the  bias equation that minimises the partial derivative in regard to error function\n",
    "\n",
    "#Gradient Descent - iterating over multiple steps with our partial weight and bias functions \n",
    "    def optimise(self): \n",
    "        learn_rate = self.learning_rate                #size of steps we make \"downhill\" to minimise total error in regards to the weight and bias \n",
    "\n",
    "        for i in range(self.epochs):                   #number of \"epochs\"/ steps we take in order to minimise aggregate error \n",
    "            self.weight = self.weight + learn_rate * self.partial_w() #optimised weight by calling partial_w as many times as epochs entered\n",
    "            self.bias = self.bias + learn_rate * self.partial_b()     #optimised bias  by calling partial_b as many times as epochs entered\n",
    "            if i % 10 == 0:                                           #prints out the weight and bias every 10 epochs \n",
    "                print(self.weight, self.bias)\n",
    "\n",
    "#DISPLAYING MODEL INTERNALS    \n",
    "#Residuals - creating a new residuals method to display deviation of predicted values from actual values\n",
    "    def residuals(self):\n",
    "        residuals = []\n",
    "        n=len(self.x) \n",
    "\n",
    "        for i in range(n):\n",
    "            residuals.append(self.y[i] - (self.weight * self.x[i] + self.bias)) #adding to the list called \"residuals\" the difference between actual and predicted y\n",
    "        for i in range(n):\n",
    "            print(float(residuals[i]))\n",
    "\n",
    "#Display values of self.x, self.y and predicted y \n",
    "    def display_x(self): #displays all of the independent variables \n",
    "        n = len(self.x)\n",
    "\n",
    "        for i in range(n):\n",
    "            print(float(self.x[i]))\n",
    "\n",
    "    def display_y(self): #displays all of the dependent variables \n",
    "        n = len(self.y)\n",
    "\n",
    "        for i in range(n):\n",
    "            print(float(self.y[i]))\n",
    "    \n",
    "    def display_predict(self): #displays model predictions \n",
    "        n = len(self.y)\n",
    "\n",
    "        for i in range(n):\n",
    "            print(float(self.y_predict[i]))\n",
    "\n",
    "    def display_weight(self):\n",
    "        return(self.weight)\n",
    "    \n",
    "    def display_bias(self):\n",
    "        return(self.bias)\n",
    "                                                                            \n",
    "#EVALUATION METRICS  -  these are key values that we will use to quantify how good our model predicts the data it is trained on. \n",
    "#Mean Squared Error (MSE)  - the average squared deviation from actual values of y\n",
    "\n",
    "    def mse(self):\n",
    "        mse = 0                              #initialising our mse as a variable  that will be updated through the loops \n",
    "        n=len(self.y)                        #creating length for range to iterate over\n",
    "        square_error = 0                #stores the square error of the deviations \n",
    "\n",
    "        for i in range(n):                   #iterating to calculate the mse \n",
    "            square_error += ((self.y[i] - (self.weight * self.x[i] + self.bias))**2)\n",
    "        mse = square_error/n                 #iterates by adding all elements in the list together to give us our aggreagte mse\n",
    "        return float(mse)\n",
    "            \n",
    "#R^2 -  how much of the deviation in y is explained by our model\n",
    "    def rsquared(self):\n",
    "        n=len(self.y)\n",
    "        self.avg_y = 0                       #initial value of the average of our actual y values \n",
    "\n",
    "    #Average y- average of our actual y    \n",
    "        for i in range(n):\n",
    "            self.avg_y += ((1/n)*self.y[i])  #calculating the average value of actual y \n",
    "        \n",
    "    #Total sum of squares   \n",
    "        self.sum_squares = 0                 #creating an object called sum_squares to be used further in the function \n",
    "        sum_squares_list =[]                 #empty list to store values of sum of squares \n",
    "        n = len(self.y)\n",
    "\n",
    "        for i in range(n):\n",
    "            sum_squares_list.append((self.y[i] - self.avg_y)**2) #the squared values of actual - predicted y  and storing them in the empty list above \n",
    "            self.sum_squares += sum_squares_list[i]              #adding togther all of the sum of squares into initial variable sum_squares \n",
    "    #Squared error \n",
    "        self.square_error = 0                \n",
    "\n",
    "        for i in range(n):\n",
    "            self.square_error +=((self.y[i] - (self.weight * self.x[i] + self.bias))**2)                           \n",
    "    #Final calculation \n",
    "        rsquared = 0                         #initialising our value of rsquared as 0 \n",
    "        n=len(self.y)\n",
    "\n",
    "        rsquared = (1-(self.square_error/self.sum_squares)) #calculating R^2 with our instances of sum of squares and square error \n",
    "        return float(rsquared) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606453a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 =LinearRegression(0.0000005, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350f7617",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.vectorise_x(x_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d71e675",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.vectorise_y(y_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f763c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.optimise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a336cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.residuals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c47907",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.display_x()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de01444e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.display_y()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818597e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.display_predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4748c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.mse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546e66e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.rsquared()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df69d58b",
   "metadata": {},
   "source": [
    "## COMAPRISON TO SCIKIT LEARN AND TENSORFLOW \n",
    "I want to compare my model to the most popular ones present in other packages.\n",
    "- This is to look inot whether or not my interpretation can hold up to these solutions\n",
    "- I also wish to learn from these packages any useful techniques for handling bivrataite data modelling \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfcec2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn \n",
    "%pip install tensorflow \n",
    "import tensorflow as tf\n",
    "import sklearn as sk\n",
    "from sklearn.linear_model import LinearRegression as lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877a5e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06803759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 100 random ages between 18 and 60\n",
    "age = np.random.randint(18, 60, size=100)\n",
    "\n",
    "# generate height based loosely on age — roughly linear but with randomness\n",
    "# each person gets a slightly different \"slope\" multiplier to make it non-perfectly linear\n",
    "random_slope = np.random.normal(2.5, 0.4, size=100)  # average slope ~2.5 with variation\n",
    "noise = np.random.normal(0, 5, size=100)             # constant variance noise (homoscedastic)\n",
    "\n",
    "height = age * random_slope + noise\n",
    "\n",
    "# create the dataframe\n",
    "df = pd.DataFrame({\n",
    "    'age': age,\n",
    "    'height': height\n",
    "})\n",
    "\n",
    "# preview\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb938b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['age']\n",
    "y = df['height']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c87f220",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5933b4",
   "metadata": {},
   "source": [
    "### MY MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed913e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = LinearRegression( 0.00015, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6066ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.vectorise_x(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41da68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.vectorise_y(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa779854",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.optimise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d77691d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mine_rsquared = my_model.rsquared() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64de0dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mine_mse = my_model.mse()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff101f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mine_weight = my_model.display_weight()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9978c2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mine_bias = my_model.display_bias()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8ffd5e",
   "metadata": {},
   "source": [
    "### SCIKIT-LEARN MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae74dde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import to spilt up the data \n",
    "from sklearn.model_selection import train_test_split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d07b558",
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting the fueatures we will use on predictions \n",
    "x_sk = df[['age']] \n",
    "y_sk = df[['height']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f70ae3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using all of the data as training data \n",
    "x_train, y_train = x_sk, y_sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bda6898",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model declaration \n",
    "sklearn_model = lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd73038",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model creation \n",
    "sklearn_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7304b6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#R^2 on training data \n",
    "rsquared_train = sklearn_model.score(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81832503",
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_coef = sklearn_model.coef_\n",
    "print(sk_coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdabad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_intercept = sklearn_model.intercept_\n",
    "print(sk_intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7c264d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025b4c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_train = mean_squared_error(x_train, y_train)\n",
    "print(mse_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab14ce18",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = pd.DataFrame({\n",
    "        \"Model name\" : [\"My model\", \"Scikit-learn 100% train\"],\n",
    "        \"Mean squared error\" : [mine_mse, mse_train],\n",
    "        \"R-squared\" : [mine_rsquared, rsquared_train],\n",
    "        \"Weight (coefficient)\" : [mine_weight, float(sk_coef)],\n",
    "        \"Bias (intercept)\" : [mine_bias, float(sk_intercept)]\n",
    "\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1ba0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad50d413",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (linreg)",
   "language": "python",
   "name": "linreg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
