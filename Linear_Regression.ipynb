{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3ec2daf",
   "metadata": {},
   "source": [
    "# LINEAR REGRESSION "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2488c407",
   "metadata": {},
   "source": [
    "## CHAPTER #1 - LINEAR REGRESSION MODEL "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923bad8d",
   "metadata": {},
   "source": [
    " ### CLASS CREATION  \n",
    " This class will contain all of the model logic.   \n",
    " The class contains the Linear regression model itself. \n",
    " Each object is a \"best fit line\"  i.e a line for each set of training data.   \n",
    "\n",
    " The attributes contain the:  \n",
    " - weight (coefficeint of the independent variable, x)  \n",
    " - bias (y-intercept) of the line \n",
    "\n",
    " The methods contain all the \"verbs\" of the model (functions that change the weight and bias), namely:  \n",
    " - how the model learns the weight and bias. \n",
    " - how the model fits the line to the data.  \n",
    " - how the line is evaluated e.g R^2 and RMSE. \n",
    " - how close our predictions are i.e residuals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdeb379",
   "metadata": {},
   "outputs": [],
   "source": [
    "class  LinearRegression(object):\n",
    "    def __init__(self, weight=0, bias=0): #initialises the attributes of the class at 0\n",
    "        self.weight = weight              #stores weight \n",
    "        self.bias = bias                  #stores bias \n",
    "        self.x = []                       #creates empty list to store our predictor variables (x)  \n",
    "        self.y = []                       #creates empty list to store our our predicted variables (y) \n",
    " \n",
    "    def vectorise(self, x,y):             #defining method to store the data points to be modelled\n",
    "        self.x = x                        #storing the values of x (independent variable) within the class \n",
    "        self.y = y                        #storing the values of y (dependent variable) within the class \n",
    "\n",
    "    def predict_y (self):                 #calculating the predicted y[i] for our optimisation later \n",
    "        y_predict =[]                     #creating an empty list to store all predicted y values \n",
    "        n = len(self.y)                   #range that we iterate over (number of values of y)\n",
    "\n",
    "        for i in range(n):                #looping over the number of values we have in the dataset \n",
    "            y_predict.append(self.weight*self.x[i] + self.bias)     #calculating predicted y values with line equation and adding predicted values to our list \n",
    "        return y_predict                  \n",
    "\n",
    "#NUMERICAL OPTIMISATION \n",
    "#Creating method to get weight\n",
    "    def partial_w(self):                  #partial derivative in regard to weight \n",
    "        y_predict = self.predict_y()      #predicted y value is equal to calling the internal method we defined above \n",
    "        gradient = 0 \n",
    "        n=len(self.y)\n",
    "\n",
    "        for i in range(n):\n",
    "            gradient += self.x[i]*(y_predict[i] - self.y[i])         #partial derivative equation to calculate total partial derivative of weight in regards to error function\n",
    "        return (-2/n)*gradient                                       #returns the  weight eqaution that minimises the partial derivative in regard to error function\n",
    "\n",
    "#Creating method to get bias \n",
    "    def partial_b(self):\n",
    "        y_predict = self.predict_y()\n",
    "        gradient = 0\n",
    "        n=len(self.y)\n",
    "\n",
    "        for i in range(n):\n",
    "            gradient += (y_predict[i]- self.y[i])                     #partial derivative equation to calculate total partial derivative of bias in regards to error function\n",
    "        return (-2/n)*gradient                                        #returns the  bias equation that minimises the partial derivative in regard to error function\n",
    "\n",
    "#Gradient Descent - iterating over multiple steps with our partial weight and bias functions \n",
    "    def optimise(self): \n",
    "        learn_rate = 0.005                 #size of steps we make \"downhill\" to minimise total error in regards to the weight and bias \n",
    "\n",
    "        for i in range(10000):             #number of \"epochs\"/ steps we take in order to minimise aggregate error \n",
    "            self.weight = self.weight + learn_rate * self.partial_w() #optimised weight by calling partial_w 10000 times\n",
    "            self.bias = self.bias + learn_rate * self.partial_b()     #optimised bias  by calling partial_b 10000 times\n",
    "            if i % 10 == 0:                #prints out the weight and bias every 10 epochs \n",
    "                print(self.weight, self.bias)\n",
    "    \n",
    "#Residuals - creating a new residuals method to display deviation of predicted values from actual values\n",
    "    def residuals(self):\n",
    "        residuals = []\n",
    "        n=len(self.x) \n",
    "\n",
    "        for i in range(n):\n",
    "            residuals.append(self.y[i] - (self.weight * self.x[i] + self.bias)) #adding to the list called \"residuals\" the difference between actual and predicted y\n",
    "        return residuals                                                        \n",
    "\n",
    "#EVALUATION METRICS  -  these are key values that we will use to quantify how good our model predicts the data it is trained on. \n",
    "#Mean Square Error (MSE)  - the average squared deviation from actual values of y\n",
    "\n",
    "    def mse(self):\n",
    "        mse = 0                              #initialising our mse as a variable  that will be updated through the loops \n",
    "        mse_list = []                        #empty list to store our mse\n",
    "        n=len(self.y)                        #creating length for range to iterate over\n",
    "        total_error = []                     #what is the total error i.e actual - predicted y\n",
    "        self.square_error = 0                #stores the square error of the deviations \n",
    "\n",
    "        for i in range(n):                   #iterating to calculate the mse \n",
    "            total_error.append((self.y[i] - (self.weight * self.x[i] + self.bias))**2) #deviation from actual y ^2\n",
    "            self.square_error += total_error[i] #storing square error to be used in future calculations\n",
    "            mse_list.append((1/n)*total_error[i]) #storing mse in the list using the mean squared error formula \n",
    "            mse += mse_list[i]               #iterates by adding all elements in the list together to give us our aggreagte mse\n",
    "        return mse \n",
    "    \n",
    "#R^2 -  how much of the deviation in y is explained by our model\n",
    "    def rsquared(self):\n",
    "        n=len(self.y)\n",
    "        self.avg_y = 0                       #initial value of the average of our actual y values \n",
    "\n",
    "    #Average y- average of our actual y    \n",
    "        for i in range(n):\n",
    "            self.avg_y += ((1/n)*self.y[i])  #calculating the average value of actual y \n",
    "        \n",
    "    #Total sum of squares -  \n",
    "        self.sum_squares = 0                 #creating an object called sum_squares to be used further in the function \n",
    "        sum_squares_list =[]                 #empty list to store values of sum of squares \n",
    "        n = len(self.y)\n",
    "\n",
    "        for i in range(n):\n",
    "            sum_squares_list.append((self.y[i] - self.avg_y)**2) #the squared values of actual - predicted y  and storing them in the empty list above \n",
    "            self.sum_squares += sum_squares_list[i]              #adding togther all of the sum of squares into initial variable sum_squares \n",
    "\n",
    "    #Final calculation \n",
    "        rsquared = 0                         #initialising our value of rsquared as 0 \n",
    "        n=len(self.y)\n",
    "\n",
    "        rsquared = (1-(self.square_error/self.sum_squares)) #calculating R^2 with our instances of sum of squares and square error \n",
    "        return rsquared \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244a6b2b",
   "metadata": {},
   "source": [
    "EXAMPLE WITH SIMPLE LISTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd657916",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1,2,3,4,5]\n",
    "y = [6,7,8,9,10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2850f30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install matplotlib\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a664c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965ff661",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866e5ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.vectorise(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e83ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953ad096",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_y()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd4758d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.residuals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa3d6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.mse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d6dd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.rsquared()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb11700",
   "metadata": {},
   "source": [
    "## CHAPTER #2 - HANDLING DATA "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c702605c",
   "metadata": {},
   "source": [
    "## Importing dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd27fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary packages \n",
    "%pip install pandas \n",
    "%pip install numpy\n",
    "%pip install matplotlib\n",
    "%pip install scikitlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad17ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f807e9",
   "metadata": {},
   "source": [
    "## FINANCE DATA \n",
    "The following datasets provide metrics partitioned by market capitalization, price, volatility, and turnover. The stock market activity metrics are partitioned by decile and the ETP metrics by quartile. \n",
    "\n",
    "I want to look into how cancellation rate (cancel to trade) is affected by stock volatility.  \n",
    "I will look at different deciles  (the market capitilsation of 10 businesses) and the Market cap decile column as well as its Volatility to see how cancellation rate changes with volatility.   \n",
    "\n",
    "**Hypothesis**- I would hypothesise that the greater the volaitilty the greater the rate of cancellation. \n",
    "\n",
    "In our data schema the following are defined:  \n",
    "**Market Cap  Decile(n)** - what is the decile_cancel_to_trade (number of cancelled trades/ number of successful trades) for that capitalisation at that date. \n",
    "-  will be renamed to \"Cancellation rate\".    \n",
    "                        \n",
    "\n",
    "**Volatility Decile(n)** - the amount of statistical variation within each stock decile (e.g decile 9) as that date. \n",
    "- will be renamed to \"Volatility\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4ddef7",
   "metadata": {},
   "source": [
    "## Can we predict the the cancelllation rate of a stock based on its volatility?\n",
    "We will use linear regression to find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a64f8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "decile_path= \"/Users/admin/Desktop/Data Science Career /Python/Python Projects/Linear regression from scratch /decile_quartile_2025_q1/decile_cancel_to_trade_stock.csv\"\n",
    "#Saving path name as variable for read csv argument "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb84cce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "decile_to_cancel_raw=pd.read_csv(decile_path) #importing file as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32907145",
   "metadata": {},
   "outputs": [],
   "source": [
    "decile_to_cancel_raw.head()\n",
    "decile_to_cancel_raw.tail()                   #insight into what our data looks like "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7e7b11",
   "metadata": {},
   "source": [
    "## EXPLORATORY DATA ANALYSIS AND DATA PRE-PROCESSING\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710dc3cb",
   "metadata": {},
   "source": [
    "I will utilise exploratory data analysis (EDA) to identify which decile has the most linear pattern in order to utilise my linear regression model.  \n",
    "This is to isolate only volatility and cancellation features for one set of independent and dependent variables. \n",
    "\n",
    "### ASSUMPTIONS OF LINEAR REGRESSION\n",
    "Let us investigate decile_1 to see if it is a good candidate to be modelled by linear regression. \n",
    "For this to be true, there must be:\n",
    "1) Strong negative or positive correlation\n",
    "2) Linearity in the data points\n",
    "3) Strong homoscedacity (data points maintian similar deviation throughtout all values of the independent variable)\n",
    "4) Normality of errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82ea148",
   "metadata": {},
   "source": [
    "#### Decile 1 and Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d77ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting features of market cap decile 1 \n",
    "#independent variable = volatility decile1 (deviation in the stock prices  in this decile for each date)\n",
    "#dependent varaible = market cap decile1 (the cancel to trade of stock each date for groups of 10 businesses in the lowest market capitalisation)\n",
    "decile_1 = decile_to_cancel_raw[[\"Market Cap Decile1\",\"Volatility Decile1\"]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06552522",
   "metadata": {},
   "outputs": [],
   "source": [
    "decile_1.head() #what does our data look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70645980",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(decile_1.rename) #help on how to rename columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b73d8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "decile_1 = decile_1.rename(columns={'Market Cap Decile1':'Cancellation rate','Volatility Decile1':'Volatility'})\n",
    "#renaming columns since we know we are in decile 1 of the decile_cancel_to_trade file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b34507f",
   "metadata": {},
   "source": [
    "## Does decile 1 fit our assumptions?\n",
    "Let us see if our data for decile 1 fits our assumptions.    \n",
    "To test this, I will make a basic plot of the two features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7600b3a",
   "metadata": {},
   "source": [
    "### Linearity\n",
    "Plot our indepdent variable vs depedent variable as a scatterplot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9f05c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1= decile_1[['Cancellation rate']] #assigning columns to the variable x\n",
    "y_1 = decile_1[['Volatility']]        #assigning columns to the variable y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86370571",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_linear = plt.scatter(y_1, x_1) #scatter plot of our two features for deile 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cfc31d",
   "metadata": {},
   "source": [
    "From this plot we can see a few key details, namely: \n",
    "1) Our data has a few outliers.  \n",
    "2) Our data does follow a linear relationship with most values condensed around the centre point of the volatility scale.  \n",
    "3) The linearity is present but does not have a strong gradient meaning the volatility in decile 1 does not have much predictive power in regards to the cancellation rate. I will explore  different features for this model i.e different deciles. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75458c33",
   "metadata": {},
   "source": [
    "#### Decile 9 and Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86fc717",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting features of market cap decile 1 \n",
    "decile_9 = decile_to_cancel_raw[[\"Market Cap Decile9\",\"Volatility Decile9\"]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7dad5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2 = decile_9[['Market Cap Decile9']] #assigning columns to the variable x\n",
    "x_2 = decile_9[['Volatility Decile9']] #assigning columns to the variable x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29181bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_linear = plt.scatter(x_2, y_2) #scatter plot of our two features for decile 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98f278a",
   "metadata": {},
   "source": [
    "From this plot we can see that:\n",
    "1) There is a strong positive linear relationship, therefore volatililty does have predictive power for cancellation rates. \n",
    "2) There is heteroscedacity in the raw data, therefore we may need to apply some kind of transformation to the data but I will make the regression model and then check for homoscedacity in the residuals. \n",
    "3) Interetsing though since this high heteroscedcaity indicates that as stock get more volatile purchasing decisions become more extreme. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a18d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "decile_9 = decile_9.rename(columns={'Market Cap Decile9':'Cancellation rate','Volatility Decile9':'Volatility'})\n",
    "#renaming columns since we know we are in decile 9 of the cancel_to_trade file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df986d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_box = plt.boxplot(x_2) #creating a boxplot of our independent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0dac6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_box = plt.boxplot(y_2) #creating a boxplot of our dependent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700d585b",
   "metadata": {},
   "source": [
    "From these plots we can see the presence of:\n",
    "1) A significant number of outliers outside the maximum range of the dataset.\n",
    "2) We will use the interquartile range method to impute these values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2b0454",
   "metadata": {},
   "source": [
    "## OUTLIER REMOVAL \n",
    "I will use the method of removing values above using upper and lower bounds based on quartiles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b075e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1- compute Q1 and Q3\n",
    "#Cancellation rates \n",
    "Q1C = decile_9['Cancellation rate'].quantile(0.25) #quartile 1 of cancellation rates \n",
    "Q3C = decile_9['Cancellation rate'].quantile(0.75) ##quartile 3 of cancellation rates \n",
    "print(Q1C)\n",
    "print(Q3C)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03413096",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Volatiltiy \n",
    "Q1V = decile_9['Volatility'].quantile(0.25)         #quartile 1 of volatility  \n",
    "Q3V = decile_9['Volatility'].quantile(0.75)         #quartile 3 of volatility  \n",
    "print(Q1V)\n",
    "print(Q3V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6006d4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2 - Compute IQR\n",
    "#Cancellation rate \n",
    "IQRC = Q3C - Q1C\n",
    "print(IQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715abaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Volatility \n",
    "IQRV= Q3V -Q1V\n",
    "print(IQRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8141978c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3 - Find the upper bound and lower bound\n",
    "#Cancellation rate\n",
    "upper_b_Canc = Q3C + 1.5*IQRC\n",
    "print(upper_b_Canc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303113a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Volatility \n",
    "upper_b_Vol = Q3V + 1.5*IQRV\n",
    "print(upper_b_Vol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6f82ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of outliers - https://www.analyticsvidhya.com/blog/2022/09/dealing-with-outliers-using-the-iqr-method/\n",
    "#Cancellation rate\n",
    "decile_9[decile_9['Cancellation rate'] > upper_b_Canc].count()\n",
    "print((70/3329)*100) #percentage of values to impute. Is not excessive ≈ 2%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec6d4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Volatility\n",
    "decile_9[decile_9['Volatility'] > upper_b_Vol].count()\n",
    "print((69/3329)*100) #percentage of values to impute. Is not excessive ≈ 2%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286683e4",
   "metadata": {},
   "source": [
    "### WINSORISATION \n",
    "A method of dealing with systematic outliers to maintain distribution of data. \n",
    "\n",
    "Through our EDA, I learned that our outliers are only present in  the upper tail , truncating them would remove information from the data and make our model have \n",
    "lower predictive power at the high end.   \n",
    "Therefore I will use Winsorisation to cap them  to the max of Q3 + 1.5*IQR - https://www.datacamp.com/tutorial/winsorized-mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865b3b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Cancellation rate \n",
    "decile_9['Cancellation rate'] = decile_9['Cancellation rate'].clip (upper = upper_b_Canc) #winsorising cancellation rate \n",
    "decile_9[decile_9['Cancellation rate'] > upper_b_Canc].count()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74f3639",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Volatility\n",
    "decile_9['Volatility'] = decile_9['Volatility'].clip (upper = upper_b_Vol)                 #winsorising volatility  \n",
    "decile_9[decile_9['Volatility'] > upper_b_Vol].count()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3555675a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New  values of x and y with Winsoration \n",
    "y_3 = decile_9[['Cancellation rate']]\n",
    "x_3 = decile_9[['Volatility']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60df10a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(x_3) #New boxplot of cancellation rate with Winsorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3eaaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(y_3) #New boxplot of volatility with Winsorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f46761",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(x_3)   #New histogram of Cancellation rate with Winsorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9137a40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_3)   #New boxplot of Volatility with Winsorisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7ca26e",
   "metadata": {},
   "source": [
    "Now that we have the pre-processed data with linearity and outliers handled we can  use our linear regression model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (linreg)",
   "language": "python",
   "name": "linreg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
